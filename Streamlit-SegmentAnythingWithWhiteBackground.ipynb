{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b637180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel\n",
    "from transformers import CLIPProcessor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236a287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./myntradataset/styles.csv\"\n",
    "df = pd.read_csv(filename, on_bad_lines=\"skip\") #some lines of the dataset fail due to excess commas\n",
    "\n",
    "available_ids = os.listdir(\"./myntradataset/images\")\n",
    "available_ids = [int(x.replace(\".jpg\",\"\")) for x in available_ids]\n",
    "df = df[df.id.isin(available_ids)] #some images are not actually available\n",
    "df = df.dropna(subset='productDisplayName')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762dda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_and_titles(filepaths, titles):\n",
    "    # Create a 4x5 grid of subplots\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
    "\n",
    "    # Loop through the filepaths and display images in the subplots\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < len(filepaths):\n",
    "            filepath = filepaths[i]\n",
    "            image = plt.imread(filepath)\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Assign a title to each subplot\n",
    "            ax.set_title(f\"{titles[i]}\", fontsize = 7)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def display_images_titles_from_ids(ids, directory):\n",
    "    df_new_index = df.copy()\n",
    "    df_new_index['id'] = df_new_index['id'].astype(str)\n",
    "    df_new_index.set_index(\"id\",inplace = True)\n",
    "    df_new_index = df_new_index.loc[ids]\n",
    "    filenames = df_new_index.index.tolist()\n",
    "    filepaths = [f\"{os.path.join(directory,filename)}.jpg\"  for filename in filenames]\n",
    "    titles =  df_new_index.productDisplayName.tolist()\n",
    "\n",
    "    display_images_and_titles(filepaths, titles)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907f07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "## This version is a modified version of the above (thanks to ChatGPT)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel\n",
    "from transformers import CLIPProcessor\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n",
    "\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "filename = \"./myntradataset/styles.csv\"\n",
    "df = pd.read_csv(filename, on_bad_lines=\"skip\")\n",
    "available_ids = os.listdir(\"./myntradataset/images\")\n",
    "available_ids = [int(x.replace(\".jpg\", \"\")) for x in available_ids]\n",
    "df = df[df.id.isin(available_ids)]\n",
    "df = df.dropna(subset='productDisplayName')\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "knn_vectors = torch.load(\"image_tensor.pt\")\n",
    "knn_labels = df.id.astype(str).tolist()\n",
    "\n",
    "import faiss\n",
    "\n",
    "vectors_np = knn_vectors.numpy()\n",
    "\n",
    "dimension = vectors_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors_np)\n",
    "\n",
    "st.set_page_config(page_title=\"test\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload your file here...\", type=['png', 'jpeg', 'jpg'])\n",
    "\n",
    "#getting SAM in\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint =\"/Users/meagankenney/IMA Bootcamp/Ebay/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "#sam.to(device=device)\n",
    "    \n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Load image using OpenCV\n",
    "    image_Seg = cv2.imdecode(np.fromstring(uploaded_file.read(), np.uint8), 1)\n",
    "    image_Seg = cv2.cvtColor(image_Seg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert image to YSBCR color space\n",
    "    #img_ysbcr = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    #image_Seg = cv2.cvtColor(image_Seg, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    st.image(image_Seg)\n",
    "    #image_Seg = cv2.cvtColor(image_Seg, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    #image = Image.open(uploaded_file)\n",
    "    \n",
    "    \n",
    "    predictor.set_image(image_Seg)\n",
    "    \n",
    "    \n",
    "    #This is where we'll want to put in Tanuj's code to get user inputs instead of these presets\n",
    "    #So you'll need to map you're image and change the 125 and 200 to lie on top of whatever item you want to segment\n",
    "    input_point = np.array([[125, 200]])\n",
    "    input_label = np.array([1])\n",
    "    \n",
    "    \n",
    "    #Making Mask\n",
    "    masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,)\n",
    "    \n",
    "    mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n",
    "    \n",
    "    masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,)\n",
    "    \n",
    "    int_mask = masks.astype(int)\n",
    "    \n",
    "    #image_Seg = cv2.cvtColor(image_Seg, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    # Extracting the width and height \n",
    "    # of the image:\n",
    "    width = image_Seg.shape[0]\n",
    "    height = image_Seg.shape[1]\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            # getting the current RGB value of pixel (i,j).\n",
    "            if int_mask[0][i][j]==0:\n",
    "                image_Seg[i][j]=(255,255,255)\n",
    "    \n",
    "    st.image(image_Seg)\n",
    "\n",
    "    custom_input = processor(text=[''], images=image_Seg, return_tensors=\"pt\", padding=True)\n",
    "    custom_output = model(**custom_input)\n",
    "    custom_embedding = custom_output.image_embeds\n",
    "    k = 20\n",
    "    D, I = index.search(custom_embedding.detach().numpy(), k)\n",
    "    neighbor_labels = [knn_labels[i] for i in I[0]]\n",
    "\n",
    "    directory = \"./myntradataset/images/\"\n",
    "\n",
    "    num_columns = 5  # Number of columns in the grid\n",
    "    num_images = len(neighbor_labels)\n",
    "    num_rows = (num_images + num_columns - 1) // num_columns\n",
    "\n",
    "    with st.container():\n",
    "        for row in range(num_rows):\n",
    "            cols = st.columns(num_columns)\n",
    "            for col in range(num_columns):\n",
    "                idx = row * num_columns + col\n",
    "                if idx < num_images:\n",
    "                    label = neighbor_labels[idx]\n",
    "                    file_path = os.path.join(directory, f\"{label}.jpg\")\n",
    "                    cols[col].image(file_path, use_column_width=True)\n",
    "                    cols[col].write(df[df['id'] == int(label)].productDisplayName.iloc[0])\n",
    "                else:\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87079735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2105e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
